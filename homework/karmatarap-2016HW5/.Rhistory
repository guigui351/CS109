pi <- 0.5
p0 <- get_p(params[1,], true_f)
p1 <- get_p(params[2,], true_f)
f_hat_qda <- pi*p1/(pi*p1 + (1-pi)*p0)
p <-true_f %>% mutate(f=f_hat_qda) %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f),
data=mutate(true_f, f=f_hat_qda),
breaks=c(0.5),color="black",lwd=1.5)
grid.arrange(true_f_plot, p, nrow=1)
library(mvtnorm)
install.packages("mvtnorm")
library(mvtnorm)
get_p <- function(params, data){
dmvnorm( cbind(data$X_1, data$X_2),
mean = c(params$avg_1, params$avg_2),
sigma = matrix( c(params$sd_1^2,
params$sd_1*params$sd_2*params$r,
params$sd_1*params$sd_2*params$r,
params$sd_2^2),2,2))
}
pi <- 0.5
p0 <- get_p(params[1,], true_f)
p1 <- get_p(params[2,], true_f)
f_hat_qda <- pi*p1/(pi*p1 + (1-pi)*p0)
p <-true_f %>% mutate(f=f_hat_qda) %>%
ggplot(aes(X_1, X_2, fill=f))  +
scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f),
data=mutate(true_f, f=f_hat_qda),
breaks=c(0.5),color="black",lwd=1.5)
grid.arrange(true_f_plot, p, nrow=1)
library(caret)
grid.arrange(true_f_plot, p, nrow=1)
install.packages("gridExtra")
grid.arrange(true_f_plot, p, nrow=1)
library(gridExtra)
grid.arrange(true_f_plot, p, nrow=1)
install.packages("pROC")
library(pROC)
roc_qda <- roc(test_set$y, pred_qda)
plot(roc_qda)
params <- train_set %>% group_by(y) %>%
summarize(avg_1 = mean(X_1), avg_2 = mean(X_2),
sd_1= sd(X_1), sd_2 = sd(X_2),
r = cor(X_1,X_2))
p0 <- get_p(params[1,], dat= test_set)
p1 <- get_p(params[2,], dat= test_set)
pi <- 0.5
pred_qda <- pi*p1/(pi*p1 + (1-pi)*p0)
test_set %>% mutate(pred=pred_qda, label=as.factor(y)) %>%
ggplot(aes(label,pred)) + geom_boxplot()
library(pROC)
roc_qda <- roc(test_set$y, pred_qda)
plot(roc_qda)
params <-params %>% mutate(sd_1 = mean(sd_1),
sd_2 = mean(sd_1),
r=mean(r))
p0 <- get_p(params[1,], dat = test_set)
p1 <- get_p(params[2,], dat = test_set)
pi <- 0.5
pred_lda <- pi*p1/(pi*p1 + (1-pi)*p0)
roc_lda <- roc(test_set$y, pred_lda)
plot(roc_qda)
plot(roc_lda, add=TRUE, col=2)
fit <- knn3(y~., data = train_set, k=5)
pred_knn_5 <- predict(fit, newdata = test_set)[,2]
roc_knn_5 <- roc(test_set$y, pred_knn_5)
plot(roc_qda)
plot(roc_knn_5, add=TRUE, col=3)
fit <- knn3(y~., data = train_set, k=51)
pred_knn_51 <- predict(fit, newdata = test_set)[,2]
roc_knn_51 <- roc(test_set$y, pred_knn_51)
plot(roc_qda)
plot(roc_knn_51, add=TRUE, col=4)
##Get the truth
dat <- digits %>% filter(label%in%c(1,2,7))
dat <- mutate(dat, label =  as.character(label))
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1,ind2)
X <- as.matrix(dat[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
dat <- mutate(dat, X_1 = X1, X_2 = X2)
y <- as.factor(dat$label)
x <- cbind(X1, X2)
library(caret)
fit <- knn3(x, y, 51)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
true_f <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = true_f, type="prob")
f1 <- loess(yhat[,1]~X_1*X_2, data=true_f,degree=1, span=1/5)$fitted
f2 <- loess(yhat[,2]~X_1*X_2, data=true_f,degree=1, span=1/5)$fitted
f7 <- loess(yhat[,3]~X_1*X_2, data=true_f,degree=1, span=1/5)$fitted
true_f <- true_f %>% mutate(f1=pmin(f1,1), f2=f2, f7=f7)
rm(dat27,X,X1,X2,fit,GS,X1s,X2s,yhat,f1, f2, f7)
p1 <- true_f %>%
ggplot(aes(X_1, X_2, fill=f1))  +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f1),
data=true_f, breaks=c(0.5),color="black",lwd=1.5) +
guides(fill=FALSE)
p2 <- true_f %>%
ggplot(aes(X_1, X_2, fill=f2))  +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f2),
data=true_f, breaks=c(0.5),color="black",lwd=1.5) +
guides(fill=FALSE)
p3 <- true_f %>%
ggplot(aes(X_1, X_2, fill=f7))  +
geom_raster()  + #guides(fill=FALSE) +
stat_contour(aes(x=X_1,y=X_2,z=f7),
data=true_f, breaks=c(0.5),color="black",lwd=1.5) +
guides(fill=FALSE)
library(gridExtra)
grid.arrange(p1,p2,p3, nrow=1)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
plotit <- function(dat, i, n=sqrt(ncol(dat)-1)){
dat <- slice(dat,i)
tmp <-  expand.grid(Row=1:n, Column=1:n) %>%
mutate(id=i, label=dat$label,
value = unlist(dat[,-1]))
tmp%>%ggplot(aes(Row, Column, fill=value)) +
geom_raster() +
scale_y_reverse() +
scale_fill_gradient(low="white", high="black") +
ggtitle(tmp$label[1])
}
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
original_dat <- read_csv(url)
original_dat <- mutate(original_dat, label = as.factor(label))
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-test.csv"
original_test<- read_csv(url)
View(original_test)
X <- sample_n(original_dat,200) %>%
arrange(label)
d <- dist(as.matrix(X[,-1]))
image(as.matrix(d))
plot(hclust(d),labels=as.character(X$label))
tmp <- slice(original_dat,1:100)
names(tmp) <- gsub("pixel","",names(tmp))
tmp <- tmp %>% mutate(obs = 1:nrow(tmp))
tmp <- tmp %>% gather(feature, value, `0`:`783`)
tmp <- tmp %>% mutate(feature = as.numeric(feature))
tmp <- tmp %>% mutate(row = feature%%28, col =floor(feature/28))
tmp <- tmp %>% mutate(row = floor(row/4), col = floor(col/4))
tmp <- tmp %>% group_by(obs, row, col)
tmp <- tmp %>% summarize(label = label[1], value = mean(value))
tmp <- tmp %>% ungroup
tmp <- tmp %>%  mutate(feature = sprintf("X_%02d_%02d",col,row))
tmp <- tmp %>%  select(-row, -col)
tmp <- tmp %>% group_by(obs) %>% spread(feature, value) %>% ungroup %>% select(-obs)
dat <- compress(original_dat)
install.packages("compress")
compress <- function(tbl, n=4){
names(tbl) <- gsub("pixel","",names(tbl))
tbl %>% mutate(obs = 1:nrow(tbl)) %>%
gather(feature, value, `0`:`783`) %>%
mutate(feature = as.numeric(feature)) %>%
mutate(row = feature%%28, col =floor(feature/28)) %>%
mutate(row = floor(row/n), col = floor(col/n)) %>%
group_by(obs, row, col)  %>%
summarize(label = label[1], value = mean(value)) %>%
ungroup %>%
mutate(feature = sprintf("X_%02d_%02d",col,row)) %>%
select(-row, -col) %>%
group_by(obs) %>% spread(feature, value) %>%
ungroup %>%
select(-obs)
}
dat <- compress(original_dat)
library(caret)
set.seed(1)
inTrain <- createDataPartition(y = dat$label,
p=0.9)$Resample
X <- dat %>% select(-label) %>% slice(inTrain) %>% as.matrix
column_means <- colMeans(X)
plot(table(round(column_means)))
plotit(original_dat,1)
keep_columns <- which(column_means>10)
train_set <- slice(dat, inTrain) %>% select(label, keep_columns+1)
test_set <- slice(dat, -inTrain) %>% select(label, keep_columns+1)
X <- sample_n(train_set,200) %>%
arrange(label)
d <- dist(as.matrix(X[,-1]))
image(as.matrix(d))
plot(hclust(d),labels=as.character(X$label))
tmp = sample_n(train_set,5000)
control <- trainControl(method='cv', number=20)
res <- train(label ~ .,
data = tmp,
method = "knn",
trControl = control,
tuneGrid=data.frame(k=seq(1,15,2)),
metric="Accuracy")
plot(res)
plot(res)
fit <- knn3(label~., train_set, k=3)
pred <- predict(fit, newdata = test_set, type="class")
tab <- table(pred, test_set$label)
confusionMatrix(tab)
control <- trainControl(method='cv', number=10, p=.2)
dat <- usa.non.null.votes %>% mutate(y=as.factor(y))
res <- train(y ~ .,
data = dat,
method = "knn",
trControl = control,
tuneLength = 1, # How fine a mesh to go on grid
tuneGrid=data.frame(k=seq(1,164,2)),
metric="Accuracy")
plot(res)
res <- train(y ~ .,
data = dat,
method = "knn",
trControl = control,
tuneLength = 1, # How fine a mesh to go on grid
tuneGrid=data.frame(k=seq(1,164,1)),
metric="Accuracy")
plot(res)
fit <- knn3(y~., dat, k=2)
pred <- predict(fit, newdata = test_set, type="class")
fit <- knn3(y~., dat, k=2)
pred <- predict(fit, newdata = test_set)
fit <- knn3(y~., data=dat, k=2)
pred <- predict(fit, newdata = test_set)
tab <- table(pred, test_set$y)
confusionMatrix(tab)
runtestKNN <- function(indata=usa.non.null.votes) {
inTrain <- createDataPartition(y = indata$y,
p=0.8)
train_set <- slice(indata, inTrain$Resample1)
test_set <- slice(indata, -inTrain$Resample1)
knn_fit <- knn3(y~.,data = train_set)
f_hat <- round(predict(knn_fit, newdata = test_set)[,2])
tab <- table(pred=f_hat, truth=test_set$y)
return(confusionMatrix(tab)$overall["Accuracy"])
}
set.seed(1)
# Re-running 10 times using default of k=5
accuracy <- replicate(10,runtestKNN())
# train with 20% of data, 10 times
control <- trainControl(method='cv', number=10, p=.2)
dat <- usa.non.null.votes %>% mutate(y=as.factor(y))
res <- train(y ~ .,
data = dat,
method = "knn",
trControl = control,
tuneLength = 1, # How fine a mesh to go on grid
tuneGrid=data.frame(k=seq(1,164,1)),
metric="Accuracy")
plot(res)
fit <- knn3(y~., data=dat, k=2)
pred <- predict(fit, newdata = test_set)
pred <- predict(fit, newdata = test_set,type="class")
train_set
test_set
inTrain <- createDataPartition(y = dat$y,
p=0.8)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
res <- train(y ~ .,
data = dat,
method = "knn",
trControl = control,
tuneLength = 1, # How fine a mesh to go on grid
tuneGrid=data.frame(k=seq(1,164,1)),
metric="Accuracy")
plot(res)
fit <- knn3(y~., data=dat, k=2)
pred <- predict(fit, newdata = test_set,type="class")
ab <- table(pred, test_set$y)
confusionMatrix(tab)
tabx <- table(pred, test_set$y)
confusionMatrix(tabx)
set.seed(1)
# Test the accuracy on k=2
inTrain <- createDataPartition(y = dat$y,
p=0.8)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
fit <- knn3(y~., data=dat, k=2)
pred <- predict(fit, newdata = test_set,type="class")
tabx <- table(pred, test_set$y)
confusionMatrix(tabx)
x_1 <- c(1,0,0,1)
x_2 <- c(2,4,6,3)
x_3 <- c(6,4,2,4)
X <- cbind(x_1, x_2, x_3)
X
X <- matrix( c(x_1, x_2, x_3), nrow=4, ncol=3)
X
X <- matrix( c(x_1, x_2, x_3), nrow=4, ncol=3,by=row())
X
X[2,3]
X[,c(1,2)]
X[c(2,3),]
X[,1]
class(X[,2:3])
class(X[,1])
X[,1,drop=FALSE]
rowMeans(X)
colMeans(X)
library(matrixStats)
colSds(X)
colRanks(X)
install.packages("matrixStats")
res
plot(res)
res
set.seed(1)
# Test the accuracy on k=2
inTrain <- createDataPartition(y = dat$y,
p=0.8)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
fit <- knn3(y~., data=dat, k=4)
pred <- predict(fit, newdata = test_set,type="class")
tabx <- table(pred, test_set$y)
confusionMatrix(tabx)
set.seed(1)
# Test the accuracy on k=2
inTrain <- createDataPartition(y = dat$y,
p=0.8)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
fit <- knn3(y~., data=dat, k=2)
pred <- predict(fit, newdata = test_set,type="class")
tabx <- table(pred, test_set$y)
confusionMatrix(tabx)
set.seed(1)
# Test the accuracy on k=2
inTrain <- createDataPartition(y = dat$y,
p=0.8)
train_set <- slice(dat, inTrain$Resample1)
test_set <- slice(dat, -inTrain$Resample1)
fit <- knn3(y~., data=dat, k=4)
pred <- predict(fit, newdata = test_set,type="class")
tabx <- table(pred, test_set$y)
confusionMatrix(tabx)
apply(X,1,mean) ##same as rowMeans
apply(X,2,mean) ##same as rowMeans
apply(X,2,function(x){
c(sum(x[1:2]), sum(x[3:4]))
})
X[ , colMeans(X)>3]
X[X<1] <- 0.5
X
X[2,3]<-NA
X[is.na(X)] <- 0
X
X - rowMeans(X)
(X - rowMeans(X)) /sd(X)
sweep(X, 2, colMeans(X))
t( t(X) - colMeans(X) )
t(X)
t(X) %*% X
crossprod(X)
solve( crossprod(X))
qr(X)
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(rafalib)
set.seed(1)
n <- 100
lim <- c(60,78)
X <- MASS::mvrnorm(n,
c(69,69),
matrix(c(9,9*0.92,9*0.92,9*1),2,2))
mypar(1,1)
plot(X, xlim=lim, ylim=lim)
points(X[1:2,], col="red", pch=16)
lines(X[1:2,],col="red")
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
install.packages("knitr")
d=dist(X)
as.matrix(d)[1,2]
X <- sweep(X, 2, colMeans(X))
##Can also do this (advanced): X <- t(t(X) - rowMeans(t(X)))
X
Z <- X[,1]
mypar(1,2)
plot(dist(X), dist(Z))
abline(0,1)
Z <-X[,2]
plot(dist(X), dist(Z))
abline(0,1)
library(rafalib)
install.packages("rafalib")
library(rafalib)
library(rafalib)
set.seed(1)
n <- 100
lim <- c(60,78)
X <- MASS::mvrnorm(n,
c(69,69),
matrix(c(9,9*0.92,9*0.92,9*1),2,2))
mypar(1,1)
plot(X, xlim=lim, ylim=lim)
points(X[1:2,], col="red", pch=16)
lines(X[1:2,],col="red")
d=dist(X)
as.matrix(d)[1,2]
X <- sweep(X, 2, colMeans(X))
##Can also do this (advanced): X <- t(t(X) - rowMeans(t(X)))
Z <- X[,1]
mypar(1,2)
plot(dist(X), dist(Z))
abline(0,1)
Z <-X[,2]
plot(dist(X), dist(Z))
abline(0,1)
Z <- X[,1]
mypar(1,1)
plot(dist(X)/sqrt(2), dist(Z))
abline(0,1)
cor(dist(X), dist(Z))
avg <- rowMeans(X) ##or (X[,1] + X[,2])/2
diff <- X[,2] - X[,1]
Z  <- cbind( avg, diff)
mypar(1,2)
lim <- lim - 69
plot(X, xlim=lim, ylim=lim)
points(X[1:2,], col="red", pch=16)
lines(X[1:2,], col="red")
plot(Z, xlim=lim, ylim=lim)
points(Z[1:2,], col="red", pch=16)
lines(Z[1:2,], col="red")
mypar(1,1)
plot(dist(X)/sqrt(2), dist(Z[,1]))
abline(0,1)
cor(dist(Z[,1]), dist(X)/sqrt(2))
prcomp(R)
prcomp(X)
Iris
View(iris)
X <- iris %>% select(-Species) %>% as.matrix()
d <- dist(X)
image(as.matrix(d))
cor(X)
pc <- prcomp(X)
summary(pc)
d_approx <- dist(pc$x[,1:2])
plot(d, d_approx)
abline(0,1, col=2)
data.frame(pc$x[,1:2], Species=iris$Species) %>%
ggplot(aes(PC1,PC2, fill = Species))+
geom_point(cex=3, pch=21) +
coord_fixed(ratio = 1)
countries <- votes %>%
filter(importantvote==1) %>%
group_by(country) %>%
summarize(p=n()/368) %>%
filter(p>=0.95) %>%
.$country
countries
dat <-  filter(polls_2008, start_date>="2008-06-01") %>%
group_by(X=day)  %>%
summarize(Y=mean(diff))
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(gganimate)
library(readr)
library(knitr)
library(broom)
library(stringr)
library(lubridate)
library(tidyr)
library(XML)
theurl <- paste0("http://www.pollster.com/08USPresGEMvO-2.html")
polls_2008 <- readHTMLTable(theurl,stringsAsFactors=FALSE)[[1]] %>%
tbl_df() %>%
separate(col=Dates, into=c("start_date","end_date"), sep="-",fill="right") %>%
mutate(end_date = ifelse(is.na(end_date), start_date, end_date)) %>%
separate(start_date, c("smonth", "sday", "syear"), sep = "/",  convert = TRUE, fill = "right")%>%
mutate(end_date = ifelse(str_count(end_date, "/") == 1, paste(smonth, end_date, sep = "/"), end_date)) %>%
mutate(end_date = mdy(end_date))  %>% mutate(syear = ifelse(is.na(syear), year(end_date), syear + 2000)) %>%
unite(start_date, smonth, sday, syear)  %>%
mutate(start_date = mdy(start_date)) %>%
separate(`N/Pop`, into=c("N","population_type"), sep="\ ", convert=TRUE, fill="left") %>%
mutate(Obama = as.numeric(Obama)/100,
McCain=as.numeric(McCain)/100,
diff = Obama - McCain,
day=as.numeric(start_date - mdy("11/04/2008")))
dat <-  filter(polls_2008, start_date>="2008-06-01") %>%
group_by(X=day)  %>%
summarize(Y=mean(diff))
mod <- ksmooth(dat$X, dat$Y, kernel="box", bandwidth = span)
bin_fit <- data.frame(X=dat$X, .fitted=mod$y)
View(bin_fit)
View(mod)
install.packages("coreNLP")
source("../r/helper_functions/parse_diego_data.R")
